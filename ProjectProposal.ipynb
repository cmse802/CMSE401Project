{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectroscopy\n",
    "\n",
    "By \"Aaron Brookhouse\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"Microscope\" src=\"http://simpleicon.com/wp-content/uploads/microscope_1.png\" width=\"20%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Abstract\n",
    "\n",
    "&#9989;  Currently I am working for a plant biology lab that is interested in using Electron Microscopes in conjunction with Machine Learning techniques to be able to automatically segment different organelles in the cell samples. Not only is machine learning used, but there is also a lot of image manipulation and also working with large 3D arrays involved for creating label data from images, images from machine learning class probability outputs, etc. I have two main goals for the project. One, is to get the image prediction software running well on the hpcc with easy scripts to use that divide up the work among multiple machines/cores. Also, I want to improve my code that works on large arrays to work better in paralell. At some point, the lab I work for wants to be able to process very large cell samples, so improving an array operation from 8 seconds to 2 seconds would be very signifigant for when the project is scaled up. It will also be nicer for my GUI program I have for people to do the array translations if they have to wait less."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Schedule\n",
    "&#9989;  provide a schedule for you to complete all of your milestones on time.  Include what you plan to work on each week from now until when the projects are due:\n",
    "\n",
    "\n",
    "* Thursday February 11 - Project Proposal Milestone Due\n",
    "* Week of February 15\n",
    "* Week of February 22 - Have my personal scripts organized, and uploaded to github as the origional, slower version\n",
    "* Week of March 1 - Have pytorch connectomics installed and prediction time measured on the hpcc\n",
    "* Week of March 8 - Essentially have the initial measuring done (want to get done early so theres more optimization time)\n",
    "* Week of March 15\n",
    "* Week of March 21\n",
    "* Week of March 22 - Have multiple pytorch_connectomics instances running on the HPCC that work together\n",
    "* Thursday March 25 - Project Part 1 Due\n",
    "* Week of March 28\n",
    "* Week of March 29 - Have half of my methods optimized\n",
    "* Week of April 5\n",
    "* Week of April 12 - Be finished with project\n",
    "* Thursday April 15 - Final Project due"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 Software Exploration\n",
    "\n",
    "There are two software packages that I am going to review and benchmark. One is Pytorch Connectomics from Harvards Visual Computing Group (https://github.com/zudi-lin/pytorch_connectomics). It is a package that is for segmentation of biological electron microscope images. I will benchmark model prediction times on one node, so I can try to make it run faster by distributing it onto multiple nodes on the HPCC.\n",
    "\n",
    "I also will be working on measuring some personal scripts. Currently I have a bunch of methods that are all over the place in a few different scripts for this project. The first step will be to organize and collect them into one script, and put that script on my github. These methods mostly operate on a lot of large arrays, or long lists. This seems perfect to speed up through openmp or cuda or something like that. For instance, we have stacks of images a few hundered tall at 1500 by 1500 resolution. If I tread certain voxels of a class as a point cloud, the cloud can have several million points. These operations to convert between different formats and such that I have written will definately be much faster if parallelization is ran. These methods are used in a program that interfaces through GUI so non-programmers can use the neural network. If I can signifigantly make them faster, it would be very helpful, especially as the projects scale in size.\n",
    "\n",
    "\n",
    "\n",
    "I will provide extensive installation instructions for the pytorch connectomics package, as you must use the github to install. Numba is installable through pip. When the project is done I will make sure there is detailed instructions to run both the origional and sped up versions on the github in a readme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Part 2 Benchmark and Optimization\n",
    "\n",
    "For the pytorch connectomics package, the package has options for GPU paralellization, however there is not currently easy ways to run across many nodes. I want to make some easy to use scripts to divide up the work of model prediction into multiple nodes and combine the results. This would be very helpful as we get bigger image scans. I'm not going to be concerned with decreasing model training times as this is a one time cost for a certain cell type and the amount of labelled data is not going to increase vastly over time. \n",
    "\n",
    "\n",
    "I am thinking about utilizing numba (numba.pydata.org/numba-doc/dev/user/) to improve my written array methods. Numba should allow for easy paralellization of code involving arrays and loops, as well as actually compiling functions to machine code. I may write my own C++ code and interface through python if that seems to have additional advantages. Currently the code is running in serial on one CPU, (although some of it is vectorized through numpy). This is clearly a limitation when working through these large arrays.\n",
    "\n",
    "The project will be sucessful if I can make the pytorch connectomics package prediction quicker through using distributed computing on the HPCC, and make it as easy as possible for non-programmers to run it.\n",
    "\n",
    "For my personal code, I will consider it succesful if I get at least 8x speedup, but I think I can probably make it even better than that considering I am currently using one core and some graphics processors have thousands of cuda cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
